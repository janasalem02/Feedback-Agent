"""
Ø£Ø¯Ø§Ø© Ù‚ÙŠØ§Ø³ Ø£Ø¯Ø§Ø¡ Ù…ÙˆØ¯ÙŠÙ„ LLaMA Storm
Performance Benchmarking Tool for LLaMA Storm Model
"""

import time
import requests
import json
from typing import Dict, List, Any
from datetime import datetime
import statistics
from pathlib import Path

class LLaMAStormBenchmark:
    def __init__(self, model_name: str = "finalend/llama-3.1-storm:8b"):
        self.model_name = model_name
        self.ollama_url = "http://localhost:11434/api/generate"
        self.results = []
        
    def run_single_test(self, prompt: str, max_tokens: int = 150) -> Dict[str, Any]:
        """ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ§Ø­Ø¯ ÙˆÙ‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        start_time = time.time()
        
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "num_predict": max_tokens,
                "temperature": 0.7,
                "top_k": 40,
                "top_p": 0.9
            }
        }
        
        try:
            response = requests.post(
                self.ollama_url,
                json=payload,
                timeout=60
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            if response.status_code == 200:
                result = response.json()
                response_text = result.get('response', '')
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                tokens_generated = len(response_text.split())
                tokens_per_second = tokens_generated / response_time if response_time > 0 else 0
                
                return {
                    "success": True,
                    "prompt": prompt[:50] + "..." if len(prompt) > 50 else prompt,
                    "response_time": response_time,
                    "tokens_generated": tokens_generated,
                    "tokens_per_second": tokens_per_second,
                    "response_length": len(response_text),
                    "status_code": response.status_code,
                    "response_preview": response_text[:100] + "..." if len(response_text) > 100 else response_text
                }
            else:
                return {
                    "success": False,
                    "prompt": prompt[:50] + "..." if len(prompt) > 50 else prompt,
                    "response_time": response_time,
                    "error": f"HTTP {response.status_code}",
                    "status_code": response.status_code
                }
                
        except Exception as e:
            end_time = time.time()
            return {
                "success": False,
                "prompt": prompt[:50] + "..." if len(prompt) > 50 else prompt,
                "response_time": end_time - start_time,
                "error": str(e),
                "status_code": None
            }
    
    def run_benchmark_suite(self) -> Dict[str, Any]:
        """ØªØ´ØºÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø´Ø§Ù…Ù„Ø© Ù…Ù† Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        print(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ {self.model_name}")
        print("=" * 60)
        
        # Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…ØªÙ†ÙˆØ¹Ø©
        test_prompts = [
            # Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù‚ØµÙŠØ±Ø©
            "Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ",
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙ„Ø®ÙŠØµ
            """Ù„Ø®Øµ Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ: ØªØ¹ØªØ¨Ø± Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„Ù…Ø§Ù„ÙŠØ© (Ø§Ù„ÙÙ†ØªÙƒ) Ù…Ù† Ø£Ø³Ø±Ø¹ Ø§Ù„Ù‚Ø·Ø§Ø¹Ø§Øª Ù†Ù…ÙˆØ§Ù‹ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ 
            Ø­ÙŠØ« ØªØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„ØªÙ…ÙˆÙŠÙ„ ÙˆØ§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ù„ØªÙ‚Ø¯ÙŠÙ… Ø®Ø¯Ù…Ø§Øª Ù…Ø§Ù„ÙŠØ© Ù…Ø¨ØªÙƒØ±Ø© ÙˆÙ…Ø±ÙŠØ­Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†. 
            ØªØ´Ù…Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø¯ÙÙˆØ¹Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©ØŒ ÙˆØ§Ù„Ø¥Ù‚Ø±Ø§Ø¶ Ø¹Ø¨Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†ØªØŒ ÙˆØ§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± Ø§Ù„Ø¢Ù„ÙŠØŒ 
            ÙˆØ§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø´ÙØ±Ø©. ØªÙ‡Ø¯Ù Ø´Ø±ÙƒØ§Øª Ø§Ù„ÙÙ†ØªÙƒ Ø¥Ù„Ù‰ ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ÙˆØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ 
            ÙˆØ²ÙŠØ§Ø¯Ø© Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ©.""",
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ­Ù„ÙŠÙ„
            "Ø­Ù„Ù„ Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ§Øª ÙˆØ³Ù„Ø¨ÙŠØ§Øª Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù† Ø¨ÙØ¹Ø¯",
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹
            "Ø§ÙƒØªØ¨ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© Ø¹Ù† Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ ÙÙŠ 50 ÙƒÙ„Ù…Ø©",
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†ØµØ§Ø¦Ø­
            "Ø£Ø¹Ø·Ù†ÙŠ 3 Ù†ØµØ§Ø¦Ø­ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© ÙÙŠ Ø§Ù„Ø¹Ù…Ù„",
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ±Ø¬Ù…Ø© ÙˆØ§Ù„ÙÙ‡Ù…
            "Translate to Arabic: Technology is rapidly changing our world"
        ]
        
        print("ğŸ“Š ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª...")
        successful_tests = []
        failed_tests = []
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\nğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± {i}/{len(test_prompts)}: {prompt[:30]}...")
            result = self.run_single_test(prompt)
            
            if result["success"]:
                successful_tests.append(result)
                print(f"âœ… Ù†Ø¬Ø­ ÙÙŠ {result['response_time']:.2f}s - {result['tokens_per_second']:.1f} tokens/s")
            else:
                failed_tests.append(result)
                print(f"âŒ ÙØ´Ù„: {result.get('error', 'Unknown error')}")
            
            self.results.append(result)
            time.sleep(1)  # ÙØªØ±Ø© Ø±Ø§Ø­Ø© Ù‚ØµÙŠØ±Ø© Ø¨ÙŠÙ† Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        if successful_tests:
            response_times = [test["response_time"] for test in successful_tests]
            tokens_per_second = [test["tokens_per_second"] for test in successful_tests]
            
            stats = {
                "total_tests": len(test_prompts),
                "successful_tests": len(successful_tests),
                "failed_tests": len(failed_tests),
                "success_rate": (len(successful_tests) / len(test_prompts)) * 100,
                "avg_response_time": statistics.mean(response_times),
                "min_response_time": min(response_times),
                "max_response_time": max(response_times),
                "median_response_time": statistics.median(response_times),
                "avg_tokens_per_second": statistics.mean(tokens_per_second),
                "min_tokens_per_second": min(tokens_per_second),
                "max_tokens_per_second": max(tokens_per_second),
                "total_tokens_generated": sum(test["tokens_generated"] for test in successful_tests)
            }
        else:
            stats = {
                "total_tests": len(test_prompts),
                "successful_tests": 0,
                "failed_tests": len(failed_tests),
                "success_rate": 0,
                "error": "Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ÙØ´Ù„Øª"
            }
        
        return {
            "model_name": self.model_name,
            "timestamp": datetime.now().isoformat(),
            "statistics": stats,
            "detailed_results": self.results
        }
    
    def print_results(self, results: Dict[str, Any]):
        """Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø³Ù‚"""
        print("\n" + "=" * 60)
        print(f"ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ {results['model_name']}")
        print("=" * 60)
        
        stats = results["statistics"]
        
        if "error" not in stats:
            print(f"ğŸ¯ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {stats['success_rate']:.1f}% ({stats['successful_tests']}/{stats['total_tests']})")
            print(f"â±ï¸  Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {stats['avg_response_time']:.2f}s")
            print(f"ğŸ“ˆ Ø£Ø³Ø±Ø¹ Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {stats['min_response_time']:.2f}s")
            print(f"ğŸ“‰ Ø£Ø¨Ø·Ø£ Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {stats['max_response_time']:.2f}s")
            print(f"ğŸ”¢ Ø§Ù„ÙˆØ³ÙŠØ·: {stats['median_response_time']:.2f}s")
            print(f"ğŸš„ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø³Ø±Ø¹Ø©: {stats['avg_tokens_per_second']:.1f} tokens/second")
            print(f"ğŸ” Ø£Ù‚ØµÙ‰ Ø³Ø±Ø¹Ø©: {stats['max_tokens_per_second']:.1f} tokens/second")
            print(f"ğŸ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©: {stats['total_tokens_generated']}")
            
            print(f"\nğŸ† ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡:")
            if stats['avg_response_time'] < 3:
                print("âš¡ Ù…Ù…ØªØ§Ø² - Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø³Ø±ÙŠØ¹Ø© Ø¬Ø¯Ø§Ù‹")
            elif stats['avg_response_time'] < 7:
                print("âœ… Ø¬ÙŠØ¯ - Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…Ù‚Ø¨ÙˆÙ„Ø©")
            else:
                print("âš ï¸ Ø¨Ø·ÙŠØ¡ - Ù‚Ø¯ ØªØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†")
                
            if stats['avg_tokens_per_second'] > 20:
                print("ğŸš€ Ø³Ø±Ø¹Ø© ØªÙˆÙ„ÙŠØ¯ Ù…Ù…ØªØ§Ø²Ø©")
            elif stats['avg_tokens_per_second'] > 10:
                print("ğŸ‘ Ø³Ø±Ø¹Ø© ØªÙˆÙ„ÙŠØ¯ Ø¬ÙŠØ¯Ø©")
            else:
                print("ğŸŒ Ø³Ø±Ø¹Ø© ØªÙˆÙ„ÙŠØ¯ Ø¨Ø·ÙŠØ¦Ø©")
        else:
            print(f"âŒ {stats['error']}")
    
    def save_results(self, results: Dict[str, Any], filename: str = None):
        """Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ù„Ù JSON"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"benchmark_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {filename}")

def run_llama_storm_benchmark():
    """ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ LLaMA Storm"""
    benchmark = LLaMAStormBenchmark()
    results = benchmark.run_benchmark_suite()
    benchmark.print_results(results)
    benchmark.save_results(results)
    return results

if __name__ == "__main__":
    run_llama_storm_benchmark()
